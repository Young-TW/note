# 進度統整

## Pull Request for LMCache 用於修復 vLLM KV Cache 寫入錯誤

前幾周一直卡在 vLLM 使用 LMCache 時，producer 無法正常寫入 KV Cache，導致 consumer 無法讀取到 KV Cache。

更換過多種 vLLM, LMCache, redis 版本組合或是更換語言模型後，仍然無法解決問題。

```sh
lmcache_producer  | (EngineCore_DP0 pid=719) [2026-01-08 16:41:10,987] LMCache WARNING: batched put error: zip() argument 2 is shorter than argument 1 (instrumented_connector.py:118:lmcache.v1.storage_backend.connector.instrumented_connector)
```

後來去改 LMCache source code 強制印出 Python 堆疊追蹤，發現 connector 呼叫的 `_prepare_params()` 不會處理不同長度的 KV Value 導致錯誤。

因此我在 LMCache 的 `_prepare_params()` 裡面加上檢查並補齊 KV Value 長度的程式碼，解決這個問題，相關 PR 如下：

[https://github.com/LMCache/LMCache/pull/2412](https://github.com/LMCache/LMCache/pull/2412)

透過檢查 vLLM 回傳的 shapes 與 dtypes 長度是否相同，若不同則補齊 dtypes 後再進行後續處理。

```python
def _prepare_params(self):
    # vLLM might return fewer dtypes than shapes (or empty dtypes).
    # We auto-fill the missing dtypes with the first available one or a default.
    final_dtypes = self.dtypes
    if len(self.dtypes) < len(self.shapes):
        missing_count = len(self.shapes) - len(self.dtypes)

        # Use the first dtype if available, otherwise default to bfloat16 (common in vLLM)
        if self.dtypes:
            fill_dtype = self.dtypes[0]
        else:
            fill_dtype = torch.bfloat16

        # Create a new list to avoid modifying the instance's state.
        final_dtypes = self.dtypes + [fill_dtype] * missing_count

    params = [self.length, int(self.fmt.value)]
    for shape, dtype in zip(self.shapes, self.dtypes, strict=True):

    for shape, dtype in zip(self.shapes, final_dtypes, strict=True):
        assert len(shape) == 4, "Shape dimension should be 4"
        params.append(DTYPE_TO_INT[dtype])
        params.append(shape[0])
```

使用修改後的 LMCache 後，producer 不再出現上述錯誤訊息，並且 consumer 也可以正常取得 KV Cache。

成功執行時使用的模型： `Llama-3.1-8B-Instruct`, `Llama-3.3-70B-Instruct`
vLLM image： `rocm/vllm-dev:nightly_main_20260112`
redis image： `bitnamilegacy/redis:7.4.2-debian-12-r6` or `redis:8.4.0`
LMCache source： `https://github.com/Young-TW/LMCache.git` commit: `c1e3cd52a23a1c458e8831d818925232bfd9986a`

```sh
lmcache_consumer  | (APIServer pid=1) INFO 01-14 22:40:57 [loggers.py:257] Engine 000: Avg prompt throughput: 404.2 tokens/s, Avg generation throughput: 2.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 49.9%, External prefix cache hit rate: 47.5%
```

## 接下來的規劃

1. 測試不同 GPU 數量 Decode/Prefill 的組合是否正常執行
2. 測量不同組合的效能表現
3. 嘗試透過腳本自動化部署 vLLM + LMCache 環境
