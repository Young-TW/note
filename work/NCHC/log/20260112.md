# 進度統整

上次卡在 vLLM 使用 LMCache 時，producer 無法正常寫入 KV Cache，導致 consumer 無法讀取到 KV Cache。

更換過多種 vLLM, LMCache, redis 版本組合或是更換語言模型後，仍然無法解決問題。

```sh
lmcache_producer  | (EngineCore_DP0 pid=719) [2026-01-08 16:41:10,987] LMCache WARNING: batched put error: zip() argument 2 is shorter than argument 1 (instrumented_connector.py:118:lmcache.v1.storage_backend.connector.instrumented_connector)
```

後來去改 LMCache source code 強制印出 Python 堆疊追蹤，發現 connector 呼叫的 `_prepare_params()` 不會處理不同長度的 KV Value 導致錯誤。

因此我在 LMCache 的 `_prepare_params()` 裡面加上檢查並補齊 KV Value 長度的程式碼，解決這個問題，相關 PR 如下：

[https://github.com/LMCache/LMCache/pull/2412](https://github.com/LMCache/LMCache/pull/2412)

使用修改後的 LMCache 後，producer 不再出現上述錯誤訊息，並且 consumer 也可以正常取得 KV Cache。

成功執行時使用的模型： `Llama-3.1-8B-Instruct`, `Llama-3.3-70B-Instruct`
vLLM image： `rocm/vllm-dev:nightly_main_20260112`
redis image： `bitnamilegacy/redis:7.4.2-debian-12-r6` or `redis:8.4.0`
LMCache source： `https://github.com/Young-TW/LMCache.git` commit: `c1e3cd52a23a1c458e8831d818925232bfd9986a`

```sh
lmcache_consumer  | (APIServer pid=1) INFO 01-14 22:40:57 [loggers.py:257] Engine 000: Avg prompt throughput: 404.2 tokens/s, Avg generation throughput: 2.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 49.9%, External prefix cache hit rate: 47.5%
```
